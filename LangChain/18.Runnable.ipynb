{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChainçš„Runnableå¯¹è±¡\n",
    "Runnableæ˜¯LangChainä¸­æœ€æ ¸å¿ƒçš„æ¥å£ï¼Œå®ƒä¸ºæ‰€æœ‰å¯æ‰§è¡Œç»„ä»¶æä¾›ç»Ÿä¸€çš„æŠ½è±¡å±‚ã€‚åœ¨LangChainè¡¨è¾¾å¼è¯­è¨€(LCEL)ä¸­ï¼ŒRunnableæ˜¯æ„å»ºå¤„ç†æµç¨‹çš„åŸºç¡€æ„ä»¶ã€‚\n",
    "\n",
    "## æ ¸å¿ƒç‰¹æ€§\n",
    "1. ç»Ÿä¸€æ¥å£: æ‰€æœ‰Runnableå¯¹è±¡éƒ½æ”¯æŒç›¸åŒçš„APIæ–¹æ³•ï¼ŒåŒ…æ‹¬invokeã€streamã€batchç­‰\n",
    "2. å¯ç»„åˆæ€§: å¯ä»¥ä½¿ç”¨|æ“ä½œç¬¦å°†å¤šä¸ªRunnableç»„ä»¶é“¾æ¥åœ¨ä¸€èµ·\n",
    "3. å¼‚æ­¥æ”¯æŒ: æ”¯æŒainvokeã€astreamç­‰å¼‚æ­¥æ–¹æ³•\n",
    "4. æµå¼å¤„ç†: é€šè¿‡streamæ–¹æ³•æ”¯æŒæµå¼è¾“å‡º\n",
    "\n",
    "## ä¸»è¦Runnableç±»å‹\n",
    "1. RunnableLambda\n",
    "ç”¨äºå°†æ™®é€šPythonå‡½æ•°åŒ…è£…ä¸ºRunnableå¯¹è±¡:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunnableLambda,å‡½æ•°è°ƒç”¨\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def func(input):\n",
    "    print(f\"{input},å‡½æ•°è°ƒç”¨\")\n",
    "\n",
    "# å°†å‡½æ•°åŒ…è£…ä¸ºRunnableå¯¹è±¡\n",
    "runnableLambda = RunnableLambda(func)\n",
    "\n",
    "runnableLambda.invoke(\"RunnableLambda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. RunnablePassthrough\n",
    "ç›´æ¥ä¼ é€’è¾“å…¥åˆ°ä¸‹ä¸€ä¸ªç»„ä»¶ï¼Œæˆ–ç”¨äºåˆå¹¶æ•°æ®:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunnablePassthrough,å‡½æ•°è°ƒç”¨\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "chain = (\n",
    "    RunnablePassthrough() |  # ä¼ é€’åŸå§‹è¾“å…¥\n",
    "    runnableLambda           # å¤„ç†è¾“å…¥\n",
    ")\n",
    "chain.invoke(\"RunnablePassthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. RunnableParallel å¹¶å‘è°ƒç”¨Runnableï¼Œå¹¶ä¸ºæ¯ä¸ªRunnableæä¾›ç›¸åŒçš„è¾“å…¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'a': 1},func1å‡½æ•°è°ƒç”¨\n",
      "\n",
      "{'a': 1},func2å‡½æ•°è°ƒç”¨\n",
      "\n",
      "{'a': 1},func3å‡½æ•°è°ƒç”¨\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'func1': {'a': 1}, 'func2': {'a': 1}, 'func3': {'a': 1}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "def func1(input):\n",
    "    print(f\"\\n{input},func1å‡½æ•°è°ƒç”¨\")\n",
    "    return input\n",
    "def func2(input):\n",
    "    print(f\"\\n{input},func2å‡½æ•°è°ƒç”¨\")\n",
    "    return input\n",
    "def func3(input):\n",
    "    print(f\"\\n{input},func3å‡½æ•°è°ƒç”¨\")\n",
    "    return input\n",
    "\n",
    "# # æ–¹å¼ä¸€\n",
    "# chain = RunnableParallel(\n",
    "#     {\n",
    "#         \"func1\": RunnableLambda(func1), \n",
    "#         \"func2\": RunnableLambda(func2), \n",
    "#         \"func3\": RunnableLambda(func3)\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# chain.invoke(1)\n",
    "\n",
    "# # æ–¹å¼äºŒ\n",
    "# chain = RunnableParallel(\n",
    "#     func1 = RunnableLambda(func1), \n",
    "#     func2 = RunnableLambda(func2), \n",
    "#     func3 = RunnableLambda(func3)\n",
    "# )\n",
    "\n",
    "# chain.invoke(1)\n",
    "\n",
    "# æ–¹å¼ä¸‰\n",
    "chain = (\n",
    "    RunnablePassthrough() |\n",
    "    {\n",
    "        \"func1\": RunnableLambda(func1), \n",
    "        \"func2\": RunnableLambda(func2), \n",
    "        \"func3\": RunnableLambda(func3)\n",
    "    }\n",
    ")\n",
    "\n",
    "chain.invoke({\"a\": 1})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. RunnableGenerator ç”¨äºæµå¼å¤„ç†å’Œç”Ÿæˆæ•°æ®:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æˆ‘æ˜¯Qwenï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„é€šä¹‰å®éªŒå®¤è‡ªä¸»ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå¸®åŠ©ä½ å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€æŠ€æœ¯æ–‡æ¡£ç­‰ï¼Œè¿˜èƒ½è¿›è¡Œé€»è¾‘æ¨ç†ã€ç¼–ç¨‹ï¼Œè¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆç­‰ã€‚æˆ‘çš„è®­ç»ƒæ•°æ®éå¸¸ä¸°å¯Œï¼Œå› æ­¤å¯ä»¥æ”¯æŒå¤šç§è¯­è¨€å’Œé¢†åŸŸçš„é—®é¢˜ã€‚\n",
      "\n",
      "å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼æˆ‘ä¼šå°½åŠ›ä¸ºä½ æä¾›æœ‰ç”¨çš„ä¿¡æ¯å’Œå»ºè®®ã€‚ğŸ˜Š"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableGenerator\n",
    "\n",
    "def streaming_parse(chunks):\n",
    "    for chunk in chunks:\n",
    "        yield chunk.content\n",
    "\n",
    "streaming_output = RunnableGenerator(streaming_parse)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen-turbo\", \n",
    "    api_key=os.getenv(\"API_KEY\"),\n",
    "    base_url=os.getenv(\"API_BASEURL\"),\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "chain = llm | streaming_output\n",
    "\n",
    "for chunk in chain.stream(\"ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 è‡ªå®šä¹‰æµç”Ÿæˆå™¨ åŸºäºæµå¼ç”Ÿæˆçš„å›ç­”æ¥è‡ªå®šä¹‰ä¸€ä¸ªè¾“å‡ºè§£æå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "é£æœº,ç›´å‡æœº,æ»‘ç¿”æœº,çƒ­æ°”çƒ,é£è‰‡,æ— äººæœº,èˆªå¤©é£æœº,èºæ—‹æ¡¨é£æœº,å–·æ°”å¼é£æœº,è¶…éŸ³é€Ÿé£æœº\n",
      "\n",
      "====================\n",
      "\n",
      "è¿”å›ä¸€ä¸ªå—\n",
      "['é£æœº']\n",
      "è¿”å›ä¸€ä¸ªå—\n",
      "['ç›´å‡æœº']\n",
      "è¿”å›ä¸€ä¸ªå—\n",
      "['çƒ­æ°”çƒ']\n",
      "è¿”å›ä¸€ä¸ªå—\n",
      "['é£è‰‡']\n",
      "è¿”å›ä¸€ä¸ªå—\n",
      "['æ»‘ç¿”æœº']\n",
      "è¿”å›ä¸€ä¸ªå—\n",
      "['æ— äººæœº']\n",
      "è¿”å›ä¸€ä¸ªå—\n",
      "['å–·æ°”å¼å…¬åŠ¡æœº']\n",
      "è¿”å›ä¸€ä¸ªå—\n",
      "['èºæ—‹æ¡¨é£æœº']\n",
      "è¿”å›ä¸€ä¸ªå—\n",
      "['æˆ˜æ–—æœº']\n",
      "æœ€åä¸€ä¸ªå—\n",
      "['èˆªå¤©é£æœº']\n",
      "\n",
      "====================\n",
      "åŒæ­¥é€»è¾‘ï¼šæ‰“å°0\n",
      "å¼€å§‹æ‰§è¡Œå¼‚æ­¥é“¾...\n",
      "\n",
      "å¼‚æ­¥è¿”å›ä¸€ä¸ªå—\n",
      "å¼‚æ­¥é“¾è¾“å‡º: ['é£æœº']åŒæ­¥é€»è¾‘ï¼šæ‰“å°1\n",
      "åŒæ­¥é€»è¾‘ï¼šæ‰“å°2\n",
      "\n",
      "å¼‚æ­¥è¿”å›ä¸€ä¸ªå—\n",
      "å¼‚æ­¥é“¾è¾“å‡º: ['ç›´å‡æœº']åŒæ­¥é€»è¾‘ï¼šæ‰“å°3\n",
      "åŒæ­¥é€»è¾‘ï¼šæ‰“å°4\n",
      "\n",
      "å¼‚æ­¥è¿”å›ä¸€ä¸ªå—\n",
      "å¼‚æ­¥é“¾è¾“å‡º: ['çƒ­æ°”çƒ']åŒæ­¥é€»è¾‘ï¼šæ‰“å°5\n",
      "åŒæ­¥é€»è¾‘ï¼šæ‰“å°6\n",
      "\n",
      "å¼‚æ­¥è¿”å›ä¸€ä¸ªå—\n",
      "å¼‚æ­¥é“¾è¾“å‡º: ['é£è‰‡']åŒæ­¥é€»è¾‘ï¼šæ‰“å°7\n",
      "åŒæ­¥é€»è¾‘ï¼šæ‰“å°8\n",
      "\n",
      "å¼‚æ­¥è¿”å›ä¸€ä¸ªå—\n",
      "å¼‚æ­¥é“¾è¾“å‡º: ['æ»‘ç¿”æœº']åŒæ­¥é€»è¾‘ï¼šæ‰“å°9\n",
      "\n",
      "å¼‚æ­¥è¿”å›ä¸€ä¸ªå—\n",
      "å¼‚æ­¥é“¾è¾“å‡º: ['æ— äººæœº']\n",
      "å¼‚æ­¥è¿”å›ä¸€ä¸ªå—\n",
      "å¼‚æ­¥é“¾è¾“å‡º: ['èˆªå¤©é£æœº']\n",
      "å¼‚æ­¥è¿”å›ä¸€ä¸ªå—\n",
      "å¼‚æ­¥é“¾è¾“å‡º: ['å–·æ°”å¼æˆ˜æ–—æœº']\n",
      "å¼‚æ­¥è¿”å›ä¸€ä¸ªå—\n",
      "å¼‚æ­¥é“¾è¾“å‡º: ['èºæ—‹æ¡¨é£æœº']\n",
      "å¼‚æ­¥è¿”å›æœ€åä¸€ä¸ªå—\n",
      "å¼‚æ­¥é“¾è¾“å‡º: ['è¶…éŸ³é€Ÿé£æœº']\n",
      "å¼‚æ­¥é“¾æ‰§è¡Œå®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import time\n",
    "from typing import AsyncIterator, Iterator, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableGenerator\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "\n",
    "# åŸºç¡€æµå¼ç”Ÿæˆå™¨\n",
    "def streaming_parse(chunks):\n",
    "    for chunk in chunks:\n",
    "        yield chunk.content\n",
    "\n",
    "streaming_output = RunnableGenerator(streaming_parse)\n",
    "\n",
    "# æç¤ºè¯\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    å›ç­”ä»¥CSVçš„æ ¼å¼è¿”å›ä¸­æ–‡åˆ—è¡¨ï¼Œä¸è¦è¿”å›å…¶ä»–å†…å®¹ã€‚\n",
    "    è¾“å‡º10ä¸ªä¸{transportation}ç±»ä¼¼çš„äº¤é€šå·¥å…·\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# å¤§æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen-turbo\", \n",
    "    api_key=os.getenv(\"API_KEY\"),\n",
    "    base_url=os.getenv(\"API_BASEURL\"),\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "print('=' * 20)\n",
    "\n",
    "chain = prompt | llm | streaming_output\n",
    "\n",
    "for chunk in chain.stream({\"transportation\" : \"é£æœº\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "    \n",
    "print('\\n')\n",
    "print('=' * 20)\n",
    "\n",
    "# è‡ªå®šä¹‰è§£æå™¨ï¼Œç”¨äºæ‹†åˆ†llmä»¤ç‰Œçš„è¿­ä»£å™¨\n",
    "# å°†ç”¨é€—å·åˆ†å‰²å­—ç¬¦ä¸²ï¼ŒæŠŠå…ƒç´ æ”¾å…¥åˆ—è¡¨ä¸­\n",
    "def split_into_list(input: Iterator[str]) -> Iterator[List[str]]:\n",
    "    #ä¿ç•™è¾“å…¥ï¼Œç›´åˆ°é€—å·æ‰è¿›è¡Œæ“ä½œ\n",
    "    buffer = ''\n",
    "    for chunk in input:\n",
    "        # å°†å½“å‰å—æ·»åŠ åˆ°ç¼“å†²åŒº\n",
    "        buffer += chunk\n",
    "        while ',' in buffer:\n",
    "            comma_index = buffer.index(',')\n",
    "            print('\\nè¿”å›ä¸€ä¸ªå—')\n",
    "            yield [buffer[:comma_index].strip()]\n",
    "            buffer = buffer[comma_index + 1:]\n",
    "    print('\\næœ€åä¸€ä¸ªå—')\n",
    "    yield[buffer.strip()]\n",
    "    \n",
    "list_chain = (prompt | llm | StrOutputParser() | split_into_list)\n",
    "\n",
    "for chunk in list_chain.stream({\"transportation\" : \"é£æœº\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "    \n",
    "\n",
    "print('\\n')\n",
    "print('=' * 20)\n",
    "\n",
    "# æ·»åŠ å¼‚æ­¥æ“ä½œ\n",
    "async def asplit_into_list(input: AsyncIterator[str]) -> AsyncIterator[List[str]]:\n",
    "    #ä¿ç•™è¾“å…¥ï¼Œç›´åˆ°é€—å·æ‰è¿›è¡Œæ“ä½œ\n",
    "    buffer = ''\n",
    "    async for chunk in input:\n",
    "        # å°†å½“å‰å—æ·»åŠ åˆ°ç¼“å†²åŒº\n",
    "        buffer += chunk\n",
    "        while ',' in buffer:\n",
    "            comma_index = buffer.index(',')\n",
    "            print('\\nå¼‚æ­¥è¿”å›ä¸€ä¸ªå—')\n",
    "            yield [buffer[:comma_index].strip()]\n",
    "            buffer = buffer[comma_index + 1:]\n",
    "    print('\\nå¼‚æ­¥è¿”å›æœ€åä¸€ä¸ªå—')\n",
    "    yield[buffer.strip()]\n",
    "\n",
    "\n",
    "alist_chain = (prompt | llm | StrOutputParser() | asplit_into_list)\n",
    "\n",
    "async def run_async_chain():\n",
    "    print(\"å¼€å§‹æ‰§è¡Œå¼‚æ­¥é“¾...\")\n",
    "    async for chunk in alist_chain.astream({\"transportation\": \"é£æœº\"}):\n",
    "        print(f\"å¼‚æ­¥é“¾è¾“å‡º: {chunk}\", end=\"\", flush=True)\n",
    "        await asyncio.sleep(1)  # æ¨¡æ‹Ÿå¼‚æ­¥å¤„ç†æ—¶é—´\n",
    "    print(\"\\nå¼‚æ­¥é“¾æ‰§è¡Œå®Œæˆ\")\n",
    "    \n",
    "async def main():\n",
    "    # å¯åŠ¨å¼‚æ­¥é“¾ä½†ä¸ç­‰å¾…å®ƒå®Œæˆ\n",
    "    async_task = asyncio.create_task(run_async_chain())\n",
    "    \n",
    "    # åœ¨å¼‚æ­¥ä»»åŠ¡è¿è¡Œçš„åŒæ—¶æ‰§è¡ŒåŒæ­¥é€»è¾‘\n",
    "    for i in range(10):\n",
    "        print(f\"åŒæ­¥é€»è¾‘ï¼šæ‰“å°{i}\")\n",
    "        await asyncio.sleep(0.5)  # ä½¿ç”¨å¼‚æ­¥å»¶è¿Ÿ\n",
    "    \n",
    "    # å¦‚æœéœ€è¦ç­‰å¾…å¼‚æ­¥ä»»åŠ¡å®Œæˆå†ç»“æŸç¨‹åº\n",
    "    await async_task\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. RunnableSequence\n",
    "å°†å¤šä¸ªRunnableç»„åˆä¸ºä¸€ä¸ªæ‰§è¡Œåºåˆ—:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "éƒ‘å·æ˜¯ä¸­å›½æ²³å—çœçš„çœä¼šåŸå¸‚ã€‚æ²³å—ä½äºä¸­å›½ä¸­éƒ¨ï¼Œæ˜¯ä¸­åæ°‘æ—çš„é‡è¦å‘ç¥¥åœ°ä¹‹ä¸€ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„å†å²æ–‡åŒ–å’Œè‡ªç„¶æ™¯è§‚ã€‚éƒ‘å·ä½œä¸ºæ²³å—çœçš„çœä¼šï¼Œæ˜¯ä¸­å›½é‡è¦çš„ç»¼åˆäº¤é€šæ¢çº½å’Œä¸­åŸç»æµåŒºçš„æ ¸å¿ƒåŸå¸‚ã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\"{location}åœ¨å“ªä¸ªçœ\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen-turbo\", \n",
    "    api_key=os.getenv(\"API_KEY\"),\n",
    "    base_url=os.getenv(\"API_BASEURL\"),\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# æ˜¾å¼åˆ›å»º\n",
    "sequence = RunnableSequence(\n",
    "    first=prompt_template,\n",
    "    middle=[llm],\n",
    "    last=output_parser\n",
    ")\n",
    "\n",
    "# æ›´å¸¸è§çš„æ˜¯ä½¿ç”¨ç®¡é“æ“ä½œç¬¦\n",
    "sequence = prompt_template | llm | output_parser\n",
    "\n",
    "result = sequence.invoke({\"location\": \"éƒ‘å·\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ä½¿ç”¨@Chainè£…é¥°å™¨ å¯ä»¥å°†ä»»æ„å‡½æ•°å˜æˆé“¾:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'éƒ‘å·ï¼šåå¤æ–‡æ˜å‘æºåœ°ï¼Œç°ä»£äº¤é€šæ¢çº½ä¸æ–‡åŒ–ååŸã€‚'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen-turbo\", \n",
    "    api_key=os.getenv(\"API_KEY\"),\n",
    "    base_url=os.getenv(\"API_BASEURL\"),\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "@chain\n",
    "def custom_chain(input):\n",
    "    prompt_template1 = ChatPromptTemplate.from_template(\"ç»™æˆ‘ä»‹ç»ä¸€ä¸‹{location}\")\n",
    "    prompt_template2 = ChatPromptTemplate.from_template(\"{intruduction}\\n\\næ€»ç»“ä¸Šé¢çš„å†…å®¹ï¼Œç²¾ç®€åˆ°20å­—ä»¥å†…\")\n",
    "    chain = (\n",
    "        prompt_template1 \n",
    "        | llm \n",
    "        | StrOutputParser() \n",
    "        |{ \"intruduction\": RunnablePassthrough() }\n",
    "        | prompt_template2 \n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke(input)\n",
    "\n",
    "custom_chain.invoke({\"location\": \"éƒ‘å·\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è°ƒç”¨æ–¹æ³•\n",
    "Runnableå¯¹è±¡æä¾›å¤šç§è°ƒç”¨æ–¹å¼:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŒæ­¥è°ƒç”¨\n",
    "result = chain.invoke({\"input\": \"query\"})\n",
    "\n",
    "# æµå¼è°ƒç”¨\n",
    "for chunk in chain.stream({\"input\": \"query\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "# æ‰¹é‡å¤„ç†\n",
    "results = chain.batch([{\"input\": \"query1\"}, {\"input\": \"query2\"}])\n",
    "\n",
    "# å¼‚æ­¥è°ƒç”¨\n",
    "result = await chain.ainvoke({\"input\": \"query\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é…ç½®ä¸å›è°ƒ\n",
    "Runnableæ”¯æŒè¿è¡Œæ—¶é…ç½®:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨ä½æ¸©åº¦è¿è¡Œ - æ›´ç¡®å®šæ€§çš„è¾“å‡º\n",
      "\n",
      " content='ä¸»è§’è‰¾ç‘å…‹Â·æ˜Ÿæ²³ï¼Œæ¥è‡ªå…¬å…ƒ3125å¹´ï¼Œæ˜¯ä¸€åå¤©èµ‹å¼‚ç¦€çš„å¤ªç©ºæ¢é™©å®¶ã€‚ä»–èº«é«˜1.9ç±³ï¼Œçš®è‚¤å‘ˆæ·¡é‡‘è‰²ï¼ŒåŒçœ¼å¦‚æ˜Ÿè¾°èˆ¬é—ªçƒç€å¹½è“å…‰èŠ’ï¼Œé¢å‰æœ‰ä¸€é“ç¥ç§˜çš„é“¶è‰²çº¹è·¯ï¼Œä»¿ä½›è¿æ¥ç€å®‡å®™çš„ç§˜å¯†ã€‚æ€§æ ¼å†·é™è€ŒåšéŸ§ï¼Œå†…å¿ƒå´å……æ»¡å¯¹æœªçŸ¥çš„å¥½å¥‡ä¸çƒ­æƒ…ã€‚\\n\\nè‰¾ç‘å…‹çš„èƒ½åŠ›æºäºåŸºå› æ”¹é€ ä¸é‡å­æ„è¯†èåˆï¼Œä»–èƒ½æ„ŸçŸ¥ç©ºé—´æ³¢åŠ¨å¹¶é¢„åˆ¤èˆªçº¿ï¼Œç”šè‡³çŸ­æš‚æ“æ§å¼•åŠ›åœºã€‚ä»–æ›¾æ˜¯åœ°çƒè”é‚¦æœ€å¹´è½»çš„æ˜Ÿé™…èˆ°é•¿ï¼Œå› ä¸€æ¬¡æ„å¤–ç©¿è¶Šè™«æ´ï¼Œå¤±å»äº†æŒšçˆ±çš„èˆ°é˜Ÿå’Œå®¶å›­ã€‚è¿™æ¬¡ç»å†è®©ä»–æ›´åŠ åšå®šæ¢ç´¢å®‡å®™çš„ä½¿å‘½ï¼Œå¯»æ‰¾æ–°çš„æ –æ¯åœ°ä»¥é¿å…äººç±»ç­ç»çš„å‘½è¿ã€‚ä»–çš„æ—…ç¨‹ä¸ä»…æ˜¯å†’é™©ï¼Œæ›´æ˜¯å¯¹è‡ªæˆ‘æ•‘èµçš„è¿½å¯»ã€‚' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 167, 'prompt_tokens': 69, 'total_tokens': 236, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-turbo', 'system_fingerprint': None, 'id': 'chatcmpl-c41e9488-816f-9d23-ae7a-7835a60d36b5', 'finish_reason': 'stop', 'logprobs': None} id='run-0ace3bb1-3ffe-4384-aa7d-c20e1fd8a5b5-0' usage_metadata={'input_tokens': 69, 'output_tokens': 167, 'total_tokens': 236, 'input_token_details': {}, 'output_token_details': {}}\n",
      "\n",
      "\\ä½¿ç”¨é«˜æ¸©åº¦è¿è¡Œ - æ›´åˆ›é€ æ€§å’Œå¤šæ ·åŒ–çš„è¾“å‡º\n",
      "\n",
      " content='ä¸»è§’åä¸ºå‡¯æ´›Â·æ˜Ÿå½±ï¼Œæ˜¯ä¸€ä½æ¥è‡ªå…¬å…ƒ3125å¹´çš„å¤ªç©ºæ¢é™©å®¶ã€‚ä»–é«˜æŒ‘ä¿®é•¿ï¼Œçš®è‚¤æ³›ç€å¾®å…‰çš„é“¶ç°è‰²ï¼ŒåŒçœ¼å¦‚æ·±é‚ƒçš„é»‘æ´ï¼Œèƒ½æ•£å‘è“ç´«è‰²å…‰èŠ’ã€‚å†·é™ä¸”å¯Œæœ‰æ™ºæ…§ï¼Œä»–æ€»èƒ½åœ¨å±æœºä¸­æ‰¾åˆ°è½¬æœºã€‚\\n\\nå‡¯æ´›æ‹¥æœ‰ä¸€ç§ç½•è§çš„â€œæ—¶ç©ºå…±é¸£â€èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ„ŸçŸ¥è¿‡å»ä¸æœªæ¥çš„ä¸€ä¸æ¶Ÿæ¼ªï¼Œå¹¶çŸ­æš‚æ“æ§æ—¶é—´æµé€Ÿã€‚ä»–çš„ç‰¹æ®ŠæŠ€èƒ½æºäºä¸€æ¬¡ç©¿è¶Šè™«æ´æ—¶é­é‡çš„èƒ½é‡çˆ†ç‚¸ï¼Œé‚£æ¬¡äº‹æ•…ä¸ä»…é‡å¡‘äº†ä»–çš„èº«ä½“ç»“æ„ï¼Œä¹Ÿèµ‹äºˆäº†ä»–è¶…å‡¡çš„è®¤çŸ¥åŠ›ã€‚\\n\\nä»–æ›¾æ˜¯æ˜Ÿé™…è”é‚¦æœ€å¹´è½»çš„èˆ°é˜ŸæŒ‡æŒ¥å®˜ï¼Œå´å› æ‹’ç»æ‰§è¡Œå¯èƒ½æ‘§æ¯æ–‡æ˜çš„å‘½ä»¤è€Œè¢«æ”¾é€ã€‚ä»æ­¤ï¼Œä»–ä»¥ç‹¬ç«‹æ¢é™©è€…çš„èº«ä»½ç©¿æ¢­å®‡å®™ï¼Œå¯»æ‰¾å¹³è¡¡ç§‘æŠ€ä¸ä¼¦ç†çš„ç­”æ¡ˆã€‚ä»–çš„ç›®æ ‡ä¸ä»…æ˜¯æ¢ç´¢æœªçŸ¥ï¼Œæ›´æ˜¯å®ˆæŠ¤é‚£äº›å³å°†æ¶ˆé€çš„æ–‡æ˜ä¹‹å…‰ã€‚' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 179, 'prompt_tokens': 69, 'total_tokens': 248, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-turbo', 'system_fingerprint': None, 'id': 'chatcmpl-ec794ea8-34fe-9897-9d64-91447d82ecc9', 'finish_reason': 'stop', 'logprobs': None} id='run-07cc1902-899b-492d-8e3f-22d9da531802-0' usage_metadata={'input_tokens': 69, 'output_tokens': 179, 'total_tokens': 248, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen-turbo\", \n",
    "    api_key=os.getenv(\"API_KEY\"),\n",
    "    base_url=os.getenv(\"API_BASEURL\")\n",
    ").configurable_fields(\n",
    "    temperature=ConfigurableField(\n",
    "        id=\"temperature\",  # é…ç½®æ ‡è¯†ç¬¦\n",
    "        name=\"Temperature\",\n",
    "        description=\"Controls randomness of the output. Higher values mean more randomness.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "ä½ æ˜¯ä¸€ä½åˆ›æ„ä½œå®¶ã€‚è¯·ä¸ºä¸€æ¬¾åä¸º'æ˜Ÿé™…è¿·èˆªè€…'çš„ç§‘å¹»æ¸¸æˆåˆ›ä½œä¸€ä¸ªä¸»è§’èƒŒæ™¯æ•…äº‹ã€‚\n",
    "è¿™ä¸ªè§’è‰²æ˜¯ä¸€ä½æ¥è‡ªæœªæ¥çš„å¤ªç©ºæ¢é™©å®¶ï¼Œæ‹¥æœ‰ç‰¹æ®Šèƒ½åŠ›ã€‚\n",
    "è¯·è¯¦ç»†æè¿°è§’è‰²çš„å¤–è²Œã€æ€§æ ¼ã€ç‰¹æ®Šèƒ½åŠ›å’Œäººç”Ÿç»å†ã€‚å­—æ•°åœ¨200å­—å·¦å³ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# ä½¿ç”¨ä½æ¸©åº¦è¿è¡Œ - æ›´ç¡®å®šæ€§çš„è¾“å‡º\n",
    "result1 = llm.invoke(prompt, config={\n",
    "    \"configurable\": {\n",
    "        \"temperature\": 0.2\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"ä½¿ç”¨ä½æ¸©åº¦è¿è¡Œ - æ›´ç¡®å®šæ€§çš„è¾“å‡º\\n\\n\",result1)\n",
    "\n",
    "# ä½¿ç”¨é«˜æ¸©åº¦è¿è¡Œ - æ›´åˆ›é€ æ€§å’Œå¤šæ ·åŒ–çš„è¾“å‡º\n",
    "result2 = llm.invoke(prompt, config={\n",
    "    \"configurable\": {\n",
    "        \"temperature\": 1.2\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"\\n\\ä½¿ç”¨é«˜æ¸©åº¦è¿è¡Œ - æ›´åˆ›é€ æ€§å’Œå¤šæ ·åŒ–çš„è¾“å‡º\\n\\n\",result2)\n",
    "\n",
    "# é…ç½®å›è°ƒ\n",
    "result3 = llm.invoke(\"ä½ å¥½\", config={\"callbacks\": [StdOutCallbackHandler()]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
