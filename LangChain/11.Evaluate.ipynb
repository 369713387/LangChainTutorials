{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估\n",
    "评估是对应用程序的输出进行质量检查的过程。由于LLM的输出具有不可预测性和多变性，判断输出是否正常比正常程序要困难。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始评估...\n",
      "生成模型回答...\n",
      "  问题 1: 已生成回答\n",
      "  问题 2: 已生成回答\n",
      "使用QAEvalChain进行评估...\n",
      "QA评估失败: chat_with_qwen() got an unexpected keyword argument 'stop'\n",
      "\n",
      "评估结果摘要:\n",
      "总评估问题数: 2\n",
      "\n",
      "平均分数:\n",
      "string_distance: 0.27\n",
      "\n",
      "详细评估结果:\n",
      "\n",
      "问题 1: 什么是机器学习？\n",
      "模型回答: 机器学习是人工智能的一个重要分支，它通过算法让计算机从数据中学习，不断改进性能。\n",
      "参考答案: 机器学习是人工智能的一个分支，它使用数据和算法来模仿人类学习的方式，逐步提高准确性。\n",
      "评估分数:\n",
      "- 字符串距离: 0.17\n",
      "\n",
      "问题 2: Python的主要特点是什么？\n",
      "模型回答: Python的主要特点是其清晰的语法和丰富的生态系统，同时它支持多种编程范式。\n",
      "参考答案: Python的主要特点包括简洁的语法、丰富的库、跨平台性和易学易用性。\n",
      "评估分数:\n",
      "- 字符串距离: 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_41744\\1658730350.py\", line 162, in evaluate_qa_system\n",
      "    qa_evals = qa_eval_chain.evaluate(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 147, in evaluate\n",
      "    return self.apply(inputs, callbacks=callbacks)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 251, in apply\n",
      "    raise e\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 248, in apply\n",
      "    response = self.generate(input_list, run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 145, in generate\n",
      "    results = self.llm.bind(stop=stop, **self.llm_kwargs).batch(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5475, in batch\n",
      "    return self.bound.batch(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 788, in batch\n",
      "    return cast(\"list[Output]\", list(executor.map(invoke, inputs, configs)))\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\concurrent\\futures\\_base.py\", line 619, in result_iterator\n",
      "    yield _result_or_cancel(fs.pop())\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\concurrent\\futures\\_base.py\", line 317, in _result_or_cancel\n",
      "    return fut.result(timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\Anaconda\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 555, in _wrapped_fn\n",
      "    return contexts.pop().run(fn, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 781, in invoke\n",
      "    return self.invoke(input, config, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 4780, in invoke\n",
      "    return self._call_with_config(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1933, in _call_with_config\n",
      "    context.run(\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 428, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 4633, in _invoke\n",
      "    output = call_func_with_variable_args(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 428, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: chat_with_qwen() got an unexpected keyword argument 'stop'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain_core.language_models import FakeListLLM\n",
    "from dashscope import Generation as DashGeneration\n",
    "\n",
    "# 读取配置文件\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# 创建与模型交互的函数\n",
    "def chat_with_qwen(prompt):\n",
    "    \"\"\"简单的函数，用于与DashScope API交互\"\"\"\n",
    "    if hasattr(prompt, \"to_string\"):\n",
    "        prompt_str = prompt.to_string()\n",
    "    else:\n",
    "        prompt_str = str(prompt)\n",
    "        \n",
    "    response = DashGeneration.call(\n",
    "        model=config['qwen_model'],\n",
    "        prompt=prompt_str,\n",
    "        api_key=config['api_key']\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.output.text\n",
    "    else:\n",
    "        raise Exception(f\"API调用失败: {response.code} - {response.message}\")\n",
    "\n",
    "# 将函数包装为Runnable对象\n",
    "qwen_chain = RunnableLambda(chat_with_qwen)\n",
    "\n",
    "# 准备评估数据\n",
    "eval_data = [\n",
    "    {\n",
    "        \"question\": \"什么是机器学习？\",\n",
    "        \"ground_truth\": \"机器学习是人工智能的一个分支，它使用数据和算法来模仿人类学习的方式，逐步提高准确性。\",\n",
    "        \"context\": \"机器学习是人工智能的一个重要分支，它通过算法让计算机从数据中学习，不断改进性能。\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Python的主要特点是什么？\",\n",
    "        \"ground_truth\": \"Python的主要特点包括简洁的语法、丰富的库、跨平台性和易学易用性。\",\n",
    "        \"context\": \"Python是一种高级编程语言，以其清晰的语法和丰富的生态系统而闻名。它支持多种编程范式。\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# 创建问答链\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "基于以下上下文回答问题：\n",
    "\n",
    "上下文：{context}\n",
    "\n",
    "问题：{question}\n",
    "\n",
    "回答：\n",
    "\"\"\")\n",
    "\n",
    "qa_chain = (\n",
    "    qa_prompt \n",
    "    | qwen_chain \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 创建QA评估链\n",
    "def create_qa_eval_chain():\n",
    "    # 创建QA评估提示模板\n",
    "    qa_eval_prompt = \"\"\"\n",
    "作为一位专业的评估专家，请评价以下问答对的质量：\n",
    "\n",
    "问题: {query}\n",
    "参考答案: {answer}\n",
    "模型回答: {result}\n",
    "\n",
    "请考虑以下几个方面：\n",
    "1. 准确性：模型回答中的信息是否与参考答案一致？\n",
    "2. 完整性：模型回答是否涵盖了参考答案中的关键信息？\n",
    "3. 相关性：模型回答是否直接回应了问题？\n",
    "\n",
    "评分:\n",
    "- 1分: 完全不正确或无关\n",
    "- 2分: 大部分不正确\n",
    "- 3分: 部分正确\n",
    "- 4分: 大部分正确\n",
    "- 5分: 完全正确\n",
    "\n",
    "请给出1-5分的评分并解释原因。\n",
    "\n",
    "评估:\n",
    "\"\"\"\n",
    "\n",
    "    # 创建QA评估链\n",
    "    eval_chain = QAEvalChain.from_llm(\n",
    "        llm=qwen_chain,\n",
    "        prompt=PromptTemplate.from_template(qa_eval_prompt)\n",
    "    )\n",
    "    \n",
    "    return eval_chain\n",
    "\n",
    "# 使用字符串距离评估器\n",
    "def get_string_distance_evaluator():\n",
    "    return load_evaluator(EvaluatorType.STRING_DISTANCE)\n",
    "\n",
    "# 评估函数\n",
    "def evaluate_qa_system(eval_data: List[Dict[str, Any]]):\n",
    "    # 创建评估器\n",
    "    qa_eval_chain = create_qa_eval_chain()\n",
    "    string_evaluator = get_string_distance_evaluator()\n",
    "    \n",
    "    results = []\n",
    "    predictions = []\n",
    "    \n",
    "    print(\"生成模型回答...\")\n",
    "    # 首先，获取所有问题的模型回答\n",
    "    for i, item in enumerate(eval_data):\n",
    "        try:\n",
    "            # 获取模型回答\n",
    "            prediction = qa_chain.invoke({\n",
    "                \"context\": item[\"context\"],\n",
    "                \"question\": item[\"question\"]\n",
    "            })\n",
    "            \n",
    "            # 为QAEvalChain准备数据\n",
    "            predictions.append({\n",
    "                \"query\": item[\"question\"],\n",
    "                \"answer\": item[\"ground_truth\"],\n",
    "                \"result\": prediction\n",
    "            })\n",
    "            \n",
    "            # 基本评估结果\n",
    "            eval_result = {\n",
    "                \"question\": item[\"question\"],\n",
    "                \"prediction\": prediction,\n",
    "                \"ground_truth\": item[\"ground_truth\"],\n",
    "                \"evaluations\": {}\n",
    "            }\n",
    "            \n",
    "            # 执行字符串距离评估\n",
    "            string_eval = string_evaluator.evaluate_strings(\n",
    "                prediction=prediction,\n",
    "                reference=item[\"ground_truth\"]\n",
    "            )\n",
    "            eval_result[\"evaluations\"][\"string_distance\"] = string_eval[\"score\"]\n",
    "            \n",
    "            results.append(eval_result)\n",
    "            print(f\"  问题 {i+1}: 已生成回答\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  问题 {i+1} 生成失败: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # 如果没有成功生成任何回答，直接返回\n",
    "    if not predictions:\n",
    "        return results\n",
    "    \n",
    "    print(\"使用QAEvalChain进行评估...\")\n",
    "    try:\n",
    "        # 使用QAEvalChain进行评估\n",
    "        qa_evals = qa_eval_chain.evaluate(\n",
    "            predictions,\n",
    "            predictions\n",
    "            )\n",
    "        \n",
    "        # 提取QA评估分数并添加到结果中\n",
    "        for i, eval_result in enumerate(results):\n",
    "            if i < len(qa_evals):\n",
    "                qa_eval = qa_evals[i]\n",
    "                # 提取评分\n",
    "                score, explanation = extract_score_from_qa_eval(qa_eval.get(\"text\", \"\"))\n",
    "                \n",
    "                eval_result[\"evaluations\"][\"qa_eval\"] = {\n",
    "                    \"score\": score,\n",
    "                    \"explanation\": explanation,\n",
    "                    \"raw\": qa_eval.get(\"text\", \"\")\n",
    "                }\n",
    "                \n",
    "                print(f\"  问题 {i+1}: 评分={score}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"QA评估失败: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 从QA评估文本中提取分数\n",
    "def extract_score_from_qa_eval(text):\n",
    "    import re\n",
    "    \n",
    "    # 尝试匹配数字评分\n",
    "    score_match = re.search(r'(\\d+)分', text)\n",
    "    if score_match:\n",
    "        try:\n",
    "            score = int(score_match.group(1))\n",
    "            if 1 <= score <= 5:\n",
    "                # 提取解释（评分后的文本）\n",
    "                explanation = text[score_match.end():].strip()\n",
    "                return score, explanation\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # 如果没有找到明确的分数，尝试根据关键词判断\n",
    "    if \"完全正确\" in text or \"非常好\" in text:\n",
    "        return 5, text\n",
    "    elif \"大部分正确\" in text:\n",
    "        return 4, text\n",
    "    elif \"部分正确\" in text:\n",
    "        return 3, text\n",
    "    elif \"大部分不正确\" in text:\n",
    "        return 2, text\n",
    "    elif \"完全不正确\" in text or \"无关\" in text:\n",
    "        return 1, text\n",
    "    \n",
    "    # 如果无法确定分数，返回一个中等分数\n",
    "    return 3, text\n",
    "\n",
    "# 结果分析函数\n",
    "def analyze_results(results: List[Dict[str, Any]]):\n",
    "    if not results:\n",
    "        return {\n",
    "            \"total_questions\": 0,\n",
    "            \"error\": \"没有可用的评估结果\"\n",
    "        }\n",
    "        \n",
    "    summary = {\n",
    "        \"total_questions\": len(results),\n",
    "        \"average_scores\": {\n",
    "            \"string_distance\": 0,\n",
    "            \"qa_eval\": 0\n",
    "        },\n",
    "        \"detailed_results\": results\n",
    "    }\n",
    "    \n",
    "    qa_eval_count = 0\n",
    "    \n",
    "    for result in results:\n",
    "        evaluations = result.get(\"evaluations\", {})\n",
    "        \n",
    "        # 字符串距离\n",
    "        if \"string_distance\" in evaluations:\n",
    "            summary[\"average_scores\"][\"string_distance\"] += evaluations[\"string_distance\"]\n",
    "        \n",
    "        # QA评估\n",
    "        if \"qa_eval\" in evaluations and \"score\" in evaluations[\"qa_eval\"]:\n",
    "            summary[\"average_scores\"][\"qa_eval\"] += evaluations[\"qa_eval\"][\"score\"]\n",
    "            qa_eval_count += 1\n",
    "    \n",
    "    # 计算平均值\n",
    "    total = len(results)\n",
    "    if total > 0:\n",
    "        summary[\"average_scores\"][\"string_distance\"] /= total\n",
    "        \n",
    "        if qa_eval_count > 0:\n",
    "            summary[\"average_scores\"][\"qa_eval\"] /= qa_eval_count\n",
    "        else:\n",
    "            # 如果没有QA评估分数，从结果中移除\n",
    "            summary[\"average_scores\"].pop(\"qa_eval\", None)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# 主评估函数\n",
    "def run_evaluation():\n",
    "    print(\"开始评估...\")\n",
    "    \n",
    "    try:\n",
    "        # 运行评估\n",
    "        results = evaluate_qa_system(eval_data)\n",
    "        \n",
    "        # 分析结果\n",
    "        summary = analyze_results(results)\n",
    "        \n",
    "        # 打印结果\n",
    "        print(\"\\n评估结果摘要:\")\n",
    "        print(f\"总评估问题数: {summary['total_questions']}\")\n",
    "        \n",
    "        if summary['total_questions'] > 0:\n",
    "            print(\"\\n平均分数:\")\n",
    "            for metric, score in summary[\"average_scores\"].items():\n",
    "                print(f\"{metric}: {score:.2f}\")\n",
    "            \n",
    "            print(\"\\n详细评估结果:\")\n",
    "            for i, result in enumerate(summary[\"detailed_results\"]):\n",
    "                print(f\"\\n问题 {i+1}: {result['question']}\")\n",
    "                print(f\"模型回答: {result['prediction']}\")\n",
    "                print(f\"参考答案: {result['ground_truth']}\")\n",
    "                \n",
    "                print(\"评估分数:\")\n",
    "                if \"string_distance\" in result[\"evaluations\"]:\n",
    "                    print(f\"- 字符串距离: {result['evaluations']['string_distance']:.2f}\")\n",
    "                \n",
    "                if \"qa_eval\" in result[\"evaluations\"]:\n",
    "                    print(f\"- QA评分: {result['evaluations']['qa_eval']['score']}\")\n",
    "                    print(f\"- 评估解释: {result['evaluations']['qa_eval']['explanation']}\")\n",
    "                    \n",
    "        else:\n",
    "            print(\"\\n没有成功的评估结果\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"评估过程中出现错误: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# 运行评估\n",
    "run_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda)",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
